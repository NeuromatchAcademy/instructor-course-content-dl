<svg id="Layer_2" data-name="Layer 2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 960 540">
  <defs>
    <style>
      .cls-1 {
        fill: #fff;
      }

      .cls-2 {
        font-size: 21px;
      }

      .cls-2, .cls-21, .cls-4 {
        font-family: Arial-BoldMT, Arial;
        font-weight: 700;
      }

      .cls-3 {
        letter-spacing: -0.037em;
      }

      .cls-19, .cls-4 {
        font-size: 12px;
      }

      .cls-11, .cls-15, .cls-16, .cls-19, .cls-5, .cls-6 {
        font-family: ArialMT, Arial;
      }

      .cls-11, .cls-15, .cls-16, .cls-5, .cls-6 {
        font-weight: 400;
      }

      .cls-14, .cls-6 {
        letter-spacing: -0.018em;
      }

      .cls-10, .cls-16, .cls-7 {
        letter-spacing: -0.055em;
      }

      .cls-10, .cls-11, .cls-15, .cls-8, .cls-9 {
        fill: blue;
      }

      .cls-10, .cls-12, .cls-15, .cls-8, .cls-9 {
        text-decoration: underline;
      }

      .cls-17, .cls-9 {
        letter-spacing: -0.074em;
      }

      .cls-13 {
        fill: #000;
      }

      .cls-18 {
        fill: #d9ead3;
      }

      .cls-18, .cls-20, .cls-24 {
        fill-rule: evenodd;
      }

      .cls-20 {
        fill: #fff2cc;
      }

      .cls-22 {
        fill: none;
        stroke: #595959;
        stroke-miterlimit: 10;
      }

      .cls-23 {
        fill: #595959;
      }

      .cls-24 {
        fill: #f4cccc;
      }
    </style>
  </defs>
  <rect class="cls-1" width="960" height="540"/>
  <text class="cls-2" transform="translate(28.733 39.292)">NM<tspan class="cls-3" x="32.659" y="0">A</tspan><tspan x="47.045" y="0" xml:space="preserve"> Robolympics: Controlling robots using reinforcement learning</tspan></text>
  <text class="cls-4" transform="translate(10.705 57.013)">Background:<tspan class="cls-5" x="74.004" y="0" xml:space="preserve"> In reinforcement learning, an agent learns to perform a task from interaction with the environment.</tspan><tspan class="cls-6" x="596.267" y="0"> </tspan><tspan class="cls-5" x="599.384" y="0">This approach, in combination with modern deep neural </tspan><tspan class="cls-5"><tspan x="0" y="14.4">networks, has been shown to be able to learn and solve a variety of games and control tasks, for example</tspan><tspan class="cls-7" x="560.982" y="14.4"> </tspan><tspan class="cls-8" x="563.654" y="14.4"><a href="https://arxiv.org/abs/1312.5602">At</a></tspan><tspan class="cls-8" x="574.992" y="14.4"><a href="https://arxiv.org/abs/1312.5602">ari g</a></tspan><tspan class="cls-8" x="598.335" y="14.4"><a href="https://arxiv.org/abs/1312.5602">ames</a></tspan><tspan x="627.679" y="14.4">, </tspan><tspan class="cls-8" x="634.347" y="14.4"><a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">Go</a></tspan><tspan x="650.355" y="14.4">, </tspan><tspan class="cls-8" x="657.023" y="14.4"><a href="https://openai.com/five/">DO</a></tspan><tspan class="cls-9" x="675.023" y="14.4"><a href="https://openai.com/five/">T</a></tspan><tspan class="cls-10" x="681.462" y="14.4"><a href="https://openai.com/five/">A</a></tspan><tspan class="cls-8" x="688.804" y="14.4" xml:space="preserve"> <a href="https://openai.com/five/">2</a></tspan><tspan x="698.812" y="14.4">, and various attempts to </tspan><tspan class="cls-8" x="833.548" y="14.4" xml:space="preserve"><a href="https://openai.com/blog/emergent-tool-use/">control embodied</a> </tspan></tspan><tspan class="cls-11"><tspan class="cls-12" x="0" y="28.8"><a href="https://openai.com/blog/emergent-tool-use/">actors</a></tspan><tspan class="cls-13" x="32.678" y="28.8" xml:space="preserve"> in physical simulations of 3D bodies. Here, we propose a collection of simple environments with robots ranging from a 2D hopper to a 3D humanoid and ask whether you </tspan></tspan><tspan class="cls-5"><tspan x="0" y="43.2">can teach a reinforcement learning agent to control the robots in order to solve a variety of Olympics-themed tasks. </tspan><tspan class="cls-14" x="612.972" y="43.2">W</tspan><tspan x="624.082" y="43.2">e mainly ask that you design a reward function suitable </tspan><tspan x="0" y="57.6">for each task and see if you can do so within some finite “electricity” cost for each movement of the robot.</tspan><tspan class="cls-14" x="557.601" y="57.6"> </tspan><tspan x="560.718" y="57.6">This project is suitable for those with some comfort in modifying an </tspan><tspan x="0" y="72">existing codebase in Python and a basic understanding of neural networks. Some ideas are presented that might be too hard to finish within 3 weeks (because many projects </tspan><tspan x="0" y="86.4">in reinforcement learning require a lot of code to create interesting environments and also take a long time to train) so please view them as mere suggestions.</tspan></tspan><tspan x="0" y="115.2">Project setup:</tspan><tspan class="cls-5" x="80.016" y="115.2"> </tspan><tspan class="cls-6" x="83.35" y="115.2">W</tspan><tspan class="cls-5" x="94.459" y="115.2">e provide a basic Jupyter </tspan><tspan class="cls-15" x="231.199" y="115.2"><a xmlns="http://www.w3.org/2000/svg" href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ReinforcementLearning/robolympics.ipynb">notebook</a></tspan><tspan class="cls-5" x="280.576" y="115.2" xml:space="preserve"> to handle creating the base environment with the robot and training a baseline network that will only learn to hop forwards </tspan><tspan class="cls-5"><tspan x="0" y="129.6">in the environment. </tspan><tspan class="cls-14" x="105.393" y="129.6">W</tspan><tspan x="116.502" y="129.6">e also include pointers to the relevant entry points in the environment codebase for you to explore and basic guidelines on how to start modifying it. </tspan><tspan class="cls-14" x="898.962" y="129.6">W</tspan><tspan x="910.072" y="129.6">e </tspan><tspan x="0" y="144">leave it to the students to implement Olympics behavior!</tspan></tspan><tspan x="0" y="172.8">Project map:</tspan><tspan class="cls-6" x="72.686" y="172.8"> </tspan><tspan class="cls-5" x="75.803" y="172.8">The project core materials will be covered in W3D2-3.</tspan><tspan class="cls-6" x="360.58" y="172.8"> </tspan><tspan class="cls-5" x="363.697" y="172.8">The progression of projects can be taken in any orde</tspan><tspan class="cls-16" x="643.863" y="172.8">r</tspan><tspan class="cls-5" x="647.197" y="172.8">, and you might find that working on one task leads to </tspan><tspan class="cls-5"><tspan x="0" y="187.2">new or unexpected project directions. Green to yellow to red represents increasing di</tspan><tspan class="cls-14" x="450.24" y="187.2">f</tspan><tspan x="453.357" y="187.2">ficult</tspan><tspan class="cls-17" x="478.031" y="187.2">y</tspan><tspan x="483.14" y="187.2">. Projects in red might require a lot of modification/programming of environments, </tspan><tspan x="0" y="201.6">and won’t necessarily be doable in 3 weeks. </tspan><tspan class="cls-14" x="237.475" y="201.6">W</tspan><tspan x="248.584" y="201.6">e include them here merely to present more ideas to start thinking about.</tspan></tspan></text>
  <g>
    <path class="cls-18" d="M29.533,263.713H315.359V334.8H29.533Z"/>
    <text class="cls-19" transform="translate(34.533 281.953)">Q1. See if you can run the example code and train <tspan x="0" y="14.4">a neural network to hop the robot forward. Can you </tspan><tspan x="0" y="28.8">improve performance or convergence by playing </tspan><tspan x="0" y="43.2">with hyperparameters (R</tspan><tspan class="cls-3" x="131.379" y="43.2">L</tspan><tspan x="137.607" y="43.2" xml:space="preserve"> algorithm, network, etc.)?</tspan></text>
  </g>
  <g>
    <path class="cls-20" d="M29.533,363.862H315.359v71.086H29.533Z"/>
    <text class="cls-19" transform="translate(34.533 374.901)">Q2. <tspan class="cls-21" x="22.676" y="0">100m dash!</tspan><tspan x="88.705" y="0" xml:space="preserve"> Make your robot sprint! Can you </tspan><tspan x="0" y="14.4">modify the reward function to be able to move at a </tspan><tspan x="0" y="28.8">specific speed and for a set distance? How fast can </tspan><tspan x="0" y="43.2">you make your robot move given some limit to the </tspan><tspan x="0" y="57.6">robot</tspan><tspan class="cls-14" x="27.352" y="57.6">’</tspan><tspan x="29.801" y="57.6">s total “energy” usage?</tspan></text>
  </g>
  <g>
    <line class="cls-22" x1="172.446" y1="334.8" x2="172.446" y2="358.28"/>
    <polygon class="cls-23" points="170.616 357.032 172.446 363.862 174.276 357.032 170.616 357.032"/>
  </g>
  <g>
    <path class="cls-24" d="M344.421,363.862H630.248v71.086H344.421Z"/>
    <text class="cls-19" transform="translate(349.421 382.101)">Q6. Can you modify the reward function and <tspan x="0" y="14.4">modify/use a di</tspan><tspan class="cls-14" x="80.707" y="14.4">f</tspan><tspan x="83.824" y="14.4">ferent environment/robot to allow </tspan><tspan x="0" y="28.8">you to move in 3D and follow a specific curved </tspan><tspan x="0" y="43.2">trajectory?</tspan></text>
  </g>
  <g>
    <path class="cls-24" d="M29.533,463.315H315.359V534.4H29.533Z"/>
    <text class="cls-19" transform="translate(34.533 474.354)">Q7. Can you improve the robustness of your agent? <tspan x="0" y="14.4">What happens if you modify the environment to add </tspan><tspan x="0" y="28.8">obstacles (e.g. falling blocks of di</tspan><tspan class="cls-14" x="176.092" y="28.8">f</tspan><tspan x="179.209" y="28.8">ferent masses)? </tspan><tspan x="0" y="43.2">Note: this will likely require a lot of modification and </tspan><tspan x="0" y="57.6">time.</tspan></text>
  </g>
  <text class="cls-19" transform="translate(646.816 493.05)">Made by Roman <tspan class="cls-17" x="91.377" y="0">V</tspan><tspan x="98.49" y="0">axenburg, Diptodip Deb, Srini</tspan><tspan class="cls-14" x="255.246" y="0"> </tspan><tspan class="cls-3" x="258.363" y="0">T</tspan><tspan x="265.248" y="0">uraga</tspan><tspan x="155.203" y="14.4">Janelia Research Campus</tspan><tspan x="51.598" y="28.8">{vaxenburg</tspan><tspan class="cls-7" x="111.645" y="28.8">r</tspan><tspan x="114.979" y="28.8">, debd, turagas}@janelia.hhmi.org</tspan></text>
  <g>
    <line class="cls-22" x1="315.359" y1="399.405" x2="338.84" y2="399.405"/>
    <polygon class="cls-23" points="337.591 401.235 344.421 399.405 337.591 397.575 337.591 401.235"/>
  </g>
  <g>
    <path class="cls-20" d="M659.452,263.713H945.279V334.8H659.452Z"/>
    <text class="cls-19" transform="translate(664.452 274.752)">Q4. <tspan class="cls-21" x="22.676" y="0">High jump!</tspan><tspan x="85.33" y="0" xml:space="preserve"> Make your robot jump! Can you </tspan><tspan x="0" y="14.4">modify the reward function/environment to be able </tspan><tspan x="0" y="28.8">to do a high jump/long jump? How high/far can you </tspan><tspan x="0" y="43.2">jump, given some limit to the robot</tspan><tspan class="cls-14" x="182.748" y="43.2">’</tspan><tspan x="185.197" y="43.2">s total “energy” </tspan><tspan x="0" y="57.6">usage?</tspan></text>
  </g>
  <g>
    <path class="cls-20" d="M344.492,263.713H630.319V334.8H344.492Z"/>
    <text class="cls-19" transform="translate(349.492 274.752)">Q3. <tspan class="cls-21" x="22.676" y="0">Marathon!</tspan><tspan x="80.672" y="0" xml:space="preserve"> Make your robot run a marathon! </tspan><tspan x="0" y="14.4">Can you modify the reward function/environment to </tspan><tspan x="0" y="28.8">be able to run/hop a marathon? How long/fast can </tspan><tspan x="0" y="43.2">you move without falling, given some limit to the </tspan><tspan x="0" y="57.6">robot</tspan><tspan class="cls-14" x="27.352" y="57.6">’</tspan><tspan x="29.801" y="57.6">s total “energy” usage?</tspan></text>
  </g>
  <g>
    <line class="cls-22" x1="315.319" y1="363.862" x2="340.538" y2="338.739"/>
    <polygon class="cls-23" points="340.946 340.917 344.492 334.8 338.362 338.323 340.946 340.917"/>
  </g>
  <g>
    <line class="cls-22" x1="344.492" y1="434.948" x2="319.273" y2="460.071"/>
    <polygon class="cls-23" points="318.866 457.893 315.319 464.01 321.45 460.487 318.866 457.893"/>
  </g>
  <g>
    <line class="cls-22" x1="630.248" y1="299.257" x2="653.728" y2="299.257"/>
    <polygon class="cls-23" points="652.48 301.087 659.309 299.257 652.48 297.426 652.48 301.087"/>
  </g>
  <g>
    <line class="cls-22" x1="487.406" y1="334.8" x2="487.406" y2="358.28"/>
    <polygon class="cls-23" points="485.575 357.032 487.406 363.862 489.236 357.032 485.575 357.032"/>
  </g>
  <g>
    <line class="cls-22" x1="659.365" y1="334.8" x2="634.146" y2="359.923"/>
    <polygon class="cls-23" points="633.739 357.745 630.192 363.862 636.322 360.338 633.739 357.745"/>
  </g>
  <g>
    <path class="cls-20" d="M659.452,363.862H945.279v71.086H659.452Z"/>
    <text class="cls-19" transform="translate(664.452 382.101)">Q5. <tspan class="cls-21" x="22.676" y="0">Competition!</tspan><tspan x="96.662" y="0" xml:space="preserve"> Pit di</tspan><tspan class="cls-14" x="126.674" y="0">f</tspan><tspan x="129.791" y="0">ferent robots against each </tspan><tspan x="0" y="14.4">other! Can you use di</tspan><tspan class="cls-14" x="114.07" y="14.4">f</tspan><tspan x="117.188" y="14.4">ferent robots to compete on </tspan><tspan x="0" y="28.8">performance? How do the behaviors/policies vary </tspan><tspan x="0" y="43.2">with energy cost, number of limbs, etc.?</tspan></text>
  </g>
  <g>
    <line class="cls-22" x1="802.366" y1="334.8" x2="802.366" y2="358.28"/>
    <polygon class="cls-23" points="800.535 357.032 802.366 363.862 804.196 357.032 800.535 357.032"/>
  </g>
</svg>