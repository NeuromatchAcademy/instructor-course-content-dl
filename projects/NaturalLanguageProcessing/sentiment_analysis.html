
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Twitter Sentiment Analysis &#8212; Neuromatch Academy: Deep Learning (instructor&#39;s version)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script >const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../../_static/nma-dl-logo-square-4xp.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Machine Translation" href="machine_translation.html" />
    <link rel="prev" title="Ideas" href="ideas_and_datasets.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/nma-dl-logo-square-4xp.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning (instructor's version)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../tutorials/intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tatraining/TA_Training_DL.html">
   TA Training: Deep Learning (DL)
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/Schedule/schedule_intro.html">
   Schedule
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/daily_schedules.html">
     General schedule
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/shared_calendars.html">
     Shared calendars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Schedule/timezone_widget.html">
     Timezone widget
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/TechnicalHelp/Discord.html">
     Using Discord
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/TechnicalHelp/Links_Policy.html">
   Quick links and policies
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs/DeepLearning.html">
   Prerequisites and preparatory materials for NMA Deep Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basics Module
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D1_BasicsAndPytorch/instructor/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/instructor/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/instructor/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/instructor/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D2_LinearDeepLearning/instructor/W1D2_BonusLecture.html">
     Bonus Lecture: Yoshua Bengio
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/instructor/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D3_MultiLayerPerceptrons/instructor/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fine Tuning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W1D5_Optimization/chapter_title.html">
   Optimization (W1D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W1D5_Optimization/instructor/W1D5_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D1_Regularization/chapter_title.html">
   Regularization (W2D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_Regularization/instructor/W2D1_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D1_Regularization/instructor/W2D1_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/Module_WrapUps/FineTuning.html">
   Deep Learning: The Basics and Fine Tuning Wrap-up
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  ConvNets and Generative Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D2_ConvnetsAndDlThinking/chapter_title.html">
   Convnets And Dl Thinking (W2D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ConvnetsAndDlThinking/instructor/W2D2_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ConvnetsAndDlThinking/instructor/W2D2_Tutorial2.html">
     Tutorial 2: Deep Learning Thinking 1: Cost Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D2_ConvnetsAndDlThinking/instructor/W2D2_BonusLecture.html">
     Bonus Lecture: Kyunghyun Cho
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D3_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D3_ModernConvnets/instructor/W2D3_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D3_ModernConvnets/instructor/W2D3_Tutorial2.html">
     Bonus Tutorial: Facial recognition using modern convnets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D4_GenerativeModels/chapter_title.html">
   Generative Models (W2D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D4_GenerativeModels/instructor/W2D4_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D4_GenerativeModels/instructor/W2D4_Tutorial2.html">
     Tutorial 2: Diffusion models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D4_GenerativeModels/instructor/W2D4_Tutorial3.html">
     Tutorial 3: Image, Conditional Diffusion and Beyond
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D4_GenerativeModels/instructor/W2D4_BonusLecture.html">
     Bonus Lecture: Geoffrey Hinton
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W2D5_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_AttentionAndTransformers/instructor/W2D5_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W2D5_AttentionAndTransformers/instructor/W2D5_Tutorial2.html">
     Bonus Tutorial: Understanding Pre-training, Fine-tuning and Robustness of Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D1_TimeSeriesAndNaturalLanguageProcessing/chapter_title.html">
   Time Series And Natural Language Processing (W3D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D1_TimeSeriesAndNaturalLanguageProcessing/instructor/W3D1_Tutorial1.html">
     Tutorial 1: Introduction to processing time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D1_TimeSeriesAndNaturalLanguageProcessing/instructor/W3D1_Tutorial2.html">
     Tutorial 2: Natural Language Processing and LLMs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D1_TimeSeriesAndNaturalLanguageProcessing/instructor/W3D1_Tutorial3.html">
     Bonus Tutorial: Multilingual Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D2_DlThinking2/chapter_title.html">
   Dl Thinking2 (W3D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D2_DlThinking2/instructor/W3D2_Tutorial1.html">
     Tutorial 1: Deep Learning Thinking 2: Architectures and Multimodal DL thinking
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/Module_WrapUps/NaturalLanguageProcessing.html">
   Deep Learning: Convnets and NLP
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Unsupervised and Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D3_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D3_UnsupervisedAndSelfSupervisedLearning/instructor/W3D3_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D3_UnsupervisedAndSelfSupervisedLearning/instructor/W3D3_BonusLecture.html">
     Bonus Lecture: Melanie Mitchell
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D4_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D4_BasicReinforcementLearning/instructor/W3D4_Tutorial1.html">
     Tutorial 1: Basic Reinforcement Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D4_BasicReinforcementLearning/instructor/W3D4_BonusLecture.html">
     Bonus Lecture: Chealsea Finn
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/chapter_title.html">
   Reinforcement Learning For Games And Dl Thinking3 (W3D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/instructor/W3D5_Tutorial1.html">
     Tutorial 1: Reinforcement Learning For Games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/instructor/W3D5_Tutorial2.html">
     Tutorial 2: Deep Learning Thinking 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/instructor/W3D5_Tutorial3.html">
     Bonus Tutorial: Planning with Monte Carlo Tree Search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/instructor/W3D5_BonusLecture.html">
     Bonus Lecture: Amita Kapoor
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deploy Models on the Web
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/Bonus_DeployModels/chapter_title.html">
   Deploy Models (Bonus)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/Bonus_DeployModels/instructor/Bonus_Tutorial1.html">
     Bonus Tutorial: Deploying Neural Networks on the Web
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Introduction to projects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/project_guidance.html">
   Daily guide for projects
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../docs/projects_overview.html">
   Project Templates
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../ComputerVision/README.html">
     Computer Vision
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
    <label for="toctree-checkbox-21">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
    <label for="toctree-checkbox-22">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../ReinforcementLearning/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../ReinforcementLearning/human_rl.html">
       Using RL to Model Cognitive Tasks
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="README.html">
     Natural Language Processing
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
    <label for="toctree-checkbox-23">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Twitter Sentiment Analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="machine_translation.html">
       Machine Translation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Neuroscience/README.html">
     Neuroscience
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
    <label for="toctree-checkbox-24">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/slides.html">
       Slides
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docs/datasets_and_models.html">
   Models and Data sets
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/NeuromatchAcademy/instructor-course-content-dl"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/NeuromatchAcademy/instructor-course-content-dl/issues/new?title=Issue%20on%20page%20%2Fprojects/NaturalLanguageProcessing/sentiment_analysis.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/projects/NaturalLanguageProcessing/sentiment_analysis.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Twitter Sentiment Analysis
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#welcome-to-the-nlp-project-template">
   Welcome to the NLP project template
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-questions-and-goals">
   Step 1: Questions and goals
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-literature-review">
   Step 2: Literature review
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-3-load-and-explore-the-dataset">
   Step 3: Load and explore the dataset
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-dependencies">
     Install dependencies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-4-choose-toolkit">
   Step 4: choose toolkit
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression">
     Logistic regression
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explainable-ai">
     Explainable AI
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-neural-network-with-pytorch">
     Recurrent Neural Network with Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-s-next">
   What’s Next?
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Twitter Sentiment Analysis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Twitter Sentiment Analysis
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#welcome-to-the-nlp-project-template">
   Welcome to the NLP project template
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-questions-and-goals">
   Step 1: Questions and goals
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-literature-review">
   Step 2: Literature review
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-3-load-and-explore-the-dataset">
   Step 3: Load and explore the dataset
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-dependencies">
     Install dependencies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-4-choose-toolkit">
   Step 4: choose toolkit
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression">
     Logistic regression
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explainable-ai">
     Explainable AI
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-neural-network-with-pytorch">
     Recurrent Neural Network with Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-s-next">
   What’s Next?
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/NaturalLanguageProcessing/sentiment_analysis.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/NaturalLanguageProcessing/sentiment_analysis.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a></p>
<div class="section" id="twitter-sentiment-analysis">
<h1>Twitter Sentiment Analysis<a class="headerlink" href="#twitter-sentiment-analysis" title="Permalink to this headline">¶</a></h1>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong>  Juan Manuel Rodriguez, Salomey Osei, Gonzalo Uribarri</p>
<p><strong>Production editors:</strong> Amita Kapoor, Spiros Chavlis</p>
</div>
<hr class="docutils" />
<div class="section" id="welcome-to-the-nlp-project-template">
<h1>Welcome to the NLP project template<a class="headerlink" href="#welcome-to-the-nlp-project-template" title="Permalink to this headline">¶</a></h1>
<img alt="https://imgs.xkcd.com/comics/machine_learning.png" src="https://imgs.xkcd.com/comics/machine_learning.png" />
</div>
<hr class="docutils" />
<div class="section" id="step-1-questions-and-goals">
<h1>Step 1: Questions and goals<a class="headerlink" href="#step-1-questions-and-goals" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Can we infer emotion from a tweet text?</p></li>
<li><p>How words are distributed accross the dataset?</p></li>
<li><p>Are words related to one kind of emotion?</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="step-2-literature-review">
<h1>Step 2: Literature review<a class="headerlink" href="#step-2-literature-review" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf">Original Dataset Paper</a></p>
<p><a class="reference external" href="https://paperswithcode.com/dataset/imdb-movie-reviews">Papers with code</a></p>
</div>
<hr class="docutils" />
<div class="section" id="step-3-load-and-explore-the-dataset">
<h1>Step 3: Load and explore the dataset<a class="headerlink" href="#step-3-load-and-explore-the-dataset" title="Permalink to this headline">¶</a></h1>
<div class="section" id="install-dependencies">
<h2>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># @title Install dependencies
!pip install pandas --quiet
!pip install torchtext --quiet
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We import some libraries to load the dataset</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torchtext</span>
<span class="kn">from</span> <span class="nn">torchtext.data</span> <span class="kn">import</span> <span class="n">get_tokenizer</span>

<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
</div>
</div>
<p>You can find the dataset we are going to use in <a class="reference external" href="http://help.sentiment140.com/for-students/">this website</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span> <span class="nn">zipfile</span><span class="o">,</span> <span class="nn">io</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip&#39;</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">))</span>
<span class="n">z</span><span class="o">.</span><span class="n">extractall</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We load the dataset</span>
<span class="n">header_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;polarity&quot;</span><span class="p">,</span> <span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;training.1600000.processed.noemoticon.csv&#39;</span><span class="p">,</span>
                 <span class="n">encoding</span> <span class="o">=</span> <span class="s2">&quot;ISO-8859-1&quot;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">header_list</span><span class="p">)</span>

<span class="c1"># Let&#39;s have a look at it</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>polarity</th>
      <th>id</th>
      <th>date</th>
      <th>query</th>
      <th>user</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1467810369</td>
      <td>Mon Apr 06 22:19:45 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>_TheSpecialOne_</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1467810672</td>
      <td>Mon Apr 06 22:19:49 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>scotthamilton</td>
      <td>is upset that he can't update his Facebook by ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1467810917</td>
      <td>Mon Apr 06 22:19:53 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>mattycus</td>
      <td>@Kenichan I dived many times for the ball. Man...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1467811184</td>
      <td>Mon Apr 06 22:19:57 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>ElleCTF</td>
      <td>my whole body feels itchy and like its on fire</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1467811193</td>
      <td>Mon Apr 06 22:19:57 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>Karoli</td>
      <td>@nationwideclass no, it's not behaving at all....</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For this project we will use only the text and the polarity of the tweet. Notice that polarity is 0 for negative tweets and 4 for positive tweet.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Changes values from [0,4] to [0,1]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">polarity</span><span class="o">.</span><span class="n">values</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>


<span class="c1"># Split the data into train and test</span>
<span class="n">x_train_text</span><span class="p">,</span> <span class="n">x_test_text</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The first thing we have to do before working on the models is to familiarize ourselves with the dataset. This is called Exploratory Data Analisys (EDA).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">]):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">s</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1: @paisleypaisley LOL why do i get ideas so far in advance? it&#39;s not even june yet! we need a third knitter to have our own summer group 
0: worst headache ever 
0: @ewaniesciuszko  i am so sad i wont see you! I miss you already. and yeah! that&#39;s perfect; i come back the 18th!
1: doesn&#39;t know how to spell conked 
0: &amp;quot;So we stand here now and no one knows us at all I won&#39;t get used to this I won&#39;t get used to being gone&amp;quot;...I miss home and everyone  -a
</pre></div>
</div>
</div>
</div>
<p>An interesting thing to analyze is the Word Distribution. In order to count the occurrences of each word, we should tokenize the sentences first.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span><span class="s2">&quot;basic_english&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before Tokenize: &#39;</span><span class="p">,</span> <span class="n">x_train_text</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After Tokenize: &#39;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before Tokenize:  worst headache ever 
After Tokenize:  [&#39;worst&#39;, &#39;headache&#39;, &#39;ever&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)]</span>
<span class="n">x_test_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">x_test_text</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "499e7fb54aa048afb3cba78dd8d6bb0e", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "fff9bd0ae74e46b0ad97ad980a834a58", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>We can count the words occurences and see how many different words are present in our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">x_train_token</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span>
    <span class="n">words</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">sorted_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">words</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">sorted_words</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">words</span><span class="p">[</span><span class="n">w</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of different Tokens in our Dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sorted_words</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sorted_words</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of different Tokens in our Dataset: 669284
[&#39;.&#39;, &#39;i&#39;, &#39;!&#39;, &quot;&#39;&quot;, &#39;to&#39;, &#39;the&#39;, &#39;,&#39;, &#39;a&#39;, &#39;my&#39;, &#39;it&#39;, &#39;and&#39;, &#39;you&#39;, &#39;?&#39;, &#39;is&#39;, &#39;for&#39;, &#39;in&#39;, &#39;s&#39;, &#39;of&#39;, &#39;t&#39;, &#39;on&#39;, &#39;that&#39;, &#39;me&#39;, &#39;so&#39;, &#39;have&#39;, &#39;m&#39;, &#39;but&#39;, &#39;just&#39;, &#39;with&#39;, &#39;be&#39;, &#39;at&#39;, &#39;not&#39;, &#39;was&#39;, &#39;this&#39;, &#39;now&#39;, &#39;can&#39;, &#39;good&#39;, &#39;up&#39;, &#39;day&#39;, &#39;all&#39;, &#39;get&#39;, &#39;out&#39;, &#39;like&#39;, &#39;are&#39;, &#39;no&#39;, &#39;go&#39;, &#39;http&#39;, &#39;-&#39;, &#39;today&#39;, &#39;do&#39;, &#39;too&#39;, &#39;your&#39;, &#39;work&#39;, &#39;going&#39;, &#39;love&#39;, &#39;we&#39;, &#39;got&#39;, &#39;what&#39;, &#39;lol&#39;, &#39;time&#39;, &#39;back&#39;, &#39;from&#39;, &#39;u&#39;, &#39;one&#39;, &#39;will&#39;, &#39;know&#39;, &#39;about&#39;, &#39;im&#39;, &#39;really&#39;, &#39;don&#39;, &#39;am&#39;, &#39;had&#39;, &#39;)&#39;, &#39;see&#39;, &#39;some&#39;, &#39;there&#39;, &#39;its&#39;, &#39;&amp;amp&#39;, &#39;how&#39;, &#39;if&#39;, &#39;still&#39;, &#39;they&#39;, &#39;&amp;quot&#39;, &#39;night&#39;, &#39;(&#39;, &#39;well&#39;, &#39;want&#39;, &#39;new&#39;, &#39;think&#39;, &#39;2&#39;, &#39;home&#39;, &#39;thanks&#39;, &#39;ll&#39;, &#39;oh&#39;, &#39;when&#39;, &#39;as&#39;, &#39;he&#39;, &#39;more&#39;, &#39;here&#39;, &#39;much&#39;, &#39;off&#39;]
</pre></div>
</div>
</div>
</div>
<p>Now we can plot their distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">count_occurences</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">words</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="n">accumulated</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">while</span> <span class="n">accumulated</span> <span class="o">&lt;</span> <span class="n">count_occurences</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">:</span>
  <span class="n">accumulated</span> <span class="o">+=</span> <span class="n">words</span><span class="p">[</span><span class="n">sorted_words</span><span class="p">[</span><span class="n">counter</span><span class="p">]]</span>
  <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">counter</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s2">% most common words &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;account for the </span><span class="si">{</span><span class="n">accumulated</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">count_occurences</span><span class="si">}</span><span class="s2">% of the occurrences&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The 0.13970153178620734% most common words account for the 80.00532743602652% of the occurrences
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sorted_words</span><span class="p">[:</span><span class="mi">100</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/sentiment_analysis_23_0.png" src="../../_images/sentiment_analysis_23_0.png" />
</div>
</div>
<p>It is very common to find this kind of distribution when analyzing corpus of text. This is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">zipf’s law</a>.</p>
<p>Usually the number of words in the dictionary will be very large.</p>
<p>Here are some thing we can do to reduce that number:</p>
<ul class="simple">
<li><p>Remove puntuation.</p></li>
<li><p>Remove stop-words.</p></li>
<li><p>Steaming.</p></li>
<li><p>Remove very uncommon words (the words that appears in fewer than N occations).</p></li>
<li><p>Nothing: we can use a pretrain model that handles this kind of situations.</p></li>
</ul>
<p>We used one of the simplest tokenizers availables. This tokenizer does not take into account many quirks of the language. Moreover, diferent languages have different quirks, so there is no “universal” tokenizers. There are many libraries that have “better” tokenizers:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spacy.io/">Spacy</a>: it can be accessed using: <code class="docutils literal notranslate"><span class="pre">get_tokenizer(&quot;spacy&quot;)</span></code>. Spacy supports a wide range of languages.</p></li>
<li><p><a class="reference external" href="https://huggingface.co/">Huggingface</a>: it has many tokenizers for different laguages. <a class="reference external" href="https://huggingface.co/transformers/main_classes/tokenizer.html">Doc</a></p></li>
<li><p><a class="reference external" href="https://www.nltk.org/">NLTK</a>: it provides several tokenizers. One of them can be accessed using: <code class="docutils literal notranslate"><span class="pre">get_tokenizer(&quot;toktok&quot;)</span></code></p></li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="step-4-choose-toolkit">
<h1>Step 4: choose toolkit<a class="headerlink" href="#step-4-choose-toolkit" title="Permalink to this headline">¶</a></h1>
<p>Our goal is to train a model capable of estimating the sentiment of a tweet (positive or negative) by reading its content. To that end we will try 2 different approaches:</p>
<ul class="simple">
<li><p>A logistic regression using sklearn. <strong>NOTE</strong>: it can probaly work better than an SVM model.</p></li>
<li><p>A simple Embedding + RNN.</p></li>
</ul>
<div class="section" id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>We will represent our senteces using binary vectorization. This means that our data would be represented as a matrix of instances by word with a one if the word is in the instance, and zero otherwise. Sklean vectorizers can also do things such as stop-word removal and puntuation removal, you can read more about in <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">the documentation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x_train_cv</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train_text</span><span class="p">)</span>
<span class="n">x_test_cv</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before Vectorize: &#39;</span><span class="p">,</span> <span class="n">x_train_text</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before Vectorize:  doesn&#39;t know how to spell conked 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Notice that the matriz is sparse</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After Vectorize: &#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_train_cv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After Vectorize: 
  (0, 528584)	1
  (0, 165468)	1
  (0, 300381)	1
  (0, 242211)	1
  (0, 489893)	1
  (0, 134160)	1
</pre></div>
</div>
</div>
</div>
<p>Now we can train our model. You can check the documentation of this logistic regressor <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic#sklearn.linear_model.LogisticRegression">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_cv</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;saga&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_cv</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.81      0.79      0.80    160000
           1       0.79      0.81      0.80    160000

    accuracy                           0.80    320000
   macro avg       0.80      0.80      0.80    320000
weighted avg       0.80      0.80      0.80    320000
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="explainable-ai">
<h2>Explainable AI<a class="headerlink" href="#explainable-ai" title="Permalink to this headline">¶</a></h2>
<p>The best thing about logistic regresion is that it is simple, and we can get some explanations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>

<span class="n">words_sk</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">words_sk</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">w</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1, 589260)
589260
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words_sk</span><span class="p">[:</span><span class="mi">20</span><span class="p">]:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">w</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>roni: -3.862597673594883
inaperfectworld: -3.5734362290886375
dontyouhate: -3.500197620227523
xbllygbsn: -3.412645372640648
anqju: -3.336405291553548
sad: -3.200522312464158
pakcricket: -3.1949158120163412
condolences: -3.132498019366488
heartbreaking: -3.066508733796654
saddest: -3.041999809733714
sadd: -3.029070563580306
heartbroken: -3.0287688233900174
boohoo: -3.022608649696793
sadface: -2.9918411285807234
rachelle_lefevr: -2.925057253107806
disappointing: -2.902524113779547
lvbu: -2.894705935001672
saddens: -2.8855127179984654
bummed: -2.83650014970307
neda: -2.792944556837498
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">words_sk</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">w</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iamsoannoyed: 2.8494314732277672
myfax: 2.797451563471618
jennamadison: 2.5667257393706113
yeyy: 2.478028598852801
tryout: 2.4383315790116677
goldymom: 2.4374026022205535
wooohooo: 2.40297322137544
thesupergirl: 2.3565118467330004
iammaxathotspot: 2.311648368632618
londicreations: 2.3074490293400993
smilin: 2.2991891636718216
worries: 2.2899429774914717
sinfulsignorita: 2.2798963640981817
finchensnail: 2.264302079155878
smackthis: 2.2376679263761083
kv: 2.2158393907798413
tojosan: 2.211784259253832
russmarshalek: 2.2095374025599384
traciknoppe: 2.1768297770350835
congratulations: 2.171590496227557
</pre></div>
</div>
</div>
</div>
<p>What does this mean?</p>
<p>Remember the <code class="docutils literal notranslate"><span class="pre">model.coef_</span></code> is the <span class="math notranslate nohighlight">\(W\)</span> in:</p>
<div class="math notranslate nohighlight">
\[h(x)=\sigma(WX + b)\]</div>
<p>where the label 1 is a positive tweet and the label 0 is a negative tweet.</p>
</div>
<div class="section" id="recurrent-neural-network-with-pytorch">
<h2>Recurrent Neural Network with Pytorch<a class="headerlink" href="#recurrent-neural-network-with-pytorch" title="Permalink to this headline">¶</a></h2>
<p>In the previous section we use a Bag-Of-Words approach to represent each of the tweets. That meas that we only consider how many times each of the words appear in each of the tweets, we didnt take into account the order of the words. But we know that the word order is very important and carries relevant information.</p>
<p>In this section we will solve the same task, but this time we will implement a Recurrent Neural Network (RNN) instead of using a simple Logistic Regression.Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences.</p>
<p>Let’s start by importing the relevant libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: For this notebook to perform best, &quot;</span>
          <span class="s2">&quot;if possible, in the menu under `Runtime` -&gt; &quot;</span>
          <span class="s2">&quot;`Change runtime type.`  select `GPU` &quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU is enabled in this notebook.&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the device (check if gpu is available)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU is enabled in this notebook.
</pre></div>
</div>
</div>
</div>
<p>First we will create a Dictionary (<code class="docutils literal notranslate"><span class="pre">word_to_idx</span></code>). This dictionary will map each Token (usually words) to an index (an integer number). We want to limit our dictionary to a certain number of tokens (<code class="docutils literal notranslate"><span class="pre">num_words_dict</span></code>), so we will include in our ditionary those with more occurrences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From previous section, we have a list with the most used tokens</span>
<span class="n">sorted_words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;.&#39;, &#39;i&#39;, &#39;!&#39;, &quot;&#39;&quot;, &#39;to&#39;, &#39;the&#39;, &#39;,&#39;, &#39;a&#39;, &#39;my&#39;, &#39;it&#39;]
</pre></div>
</div>
</div>
</div>
<p>Let’s select only the most used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_words_dict</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="c1"># We reserve two numbers for special tokens.</span>
<span class="n">most_used_words</span> <span class="o">=</span> <span class="n">sorted_words</span><span class="p">[:</span><span class="n">num_words_dict</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We will add two extra Tokens to the dictionary, one for words outside the dictionary (<code class="docutils literal notranslate"><span class="pre">'UNK'</span></code>) and one for padding the sequences (<code class="docutils literal notranslate"><span class="pre">'PAD'</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># dictionary to go from words to idx</span>
<span class="n">word_to_idx</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># dictionary to go from idx to words (just in case)</span>
<span class="n">idx_to_word</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># We include the special tokens first</span>
<span class="n">PAD_token</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">UNK_token</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">word_to_idx</span><span class="p">[</span><span class="s1">&#39;PAD&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">PAD_token</span>
<span class="n">word_to_idx</span><span class="p">[</span><span class="s1">&#39;UNK&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">UNK_token</span>

<span class="n">idx_to_word</span><span class="p">[</span><span class="n">PAD_token</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;PAD&#39;</span>
<span class="n">idx_to_word</span><span class="p">[</span><span class="n">UNK_token</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;UNK&#39;</span>

<span class="c1"># We popullate our dictionaries with the most used words</span>
<span class="k">for</span> <span class="n">num</span><span class="p">,</span><span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">most_used_words</span><span class="p">):</span>
  <span class="n">word_to_idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">num</span> <span class="o">+</span> <span class="mi">2</span>
  <span class="n">idx_to_word</span><span class="p">[</span><span class="n">num</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
</pre></div>
</div>
</div>
</div>
<p>Our goal now is to transform each tweet from a sequence of tokens to a sequence of indexes. These sequences of indexes will be the input to our pytorch model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A function to convert list of tokens to list of indexes</span>
<span class="k">def</span> <span class="nf">tokens_to_idx</span><span class="p">(</span><span class="n">sentences_tokens</span><span class="p">,</span><span class="n">word_to_idx</span><span class="p">):</span>
  <span class="n">sentences_idx</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences_tokens</span><span class="p">:</span>
    <span class="n">sent_idx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_to_idx</span><span class="p">:</span>
        <span class="n">sent_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_to_idx</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">sent_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_to_idx</span><span class="p">[</span><span class="s1">&#39;UNK&#39;</span><span class="p">])</span>
    <span class="n">sentences_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sent_idx</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">sentences_idx</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train_idx</span> <span class="o">=</span> <span class="n">tokens_to_idx</span><span class="p">(</span><span class="n">x_train_token</span><span class="p">,</span><span class="n">word_to_idx</span><span class="p">)</span>
<span class="n">x_test_idx</span> <span class="o">=</span> <span class="n">tokens_to_idx</span><span class="p">(</span><span class="n">x_test_token</span><span class="p">,</span><span class="n">word_to_idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">some_number</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before converting: &#39;</span><span class="p">,</span> <span class="n">x_train_token</span><span class="p">[</span><span class="n">some_number</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After converting: &#39;</span><span class="p">,</span> <span class="n">x_train_idx</span><span class="p">[</span><span class="n">some_number</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before converting:  [&#39;worst&#39;, &#39;headache&#39;, &#39;ever&#39;]
After converting:  [721, 458, 237]
</pre></div>
</div>
</div>
</div>
<p>We need all the sequences to have the same length. To select an adequate sequence length, let’s explore some statistics about the length of the tweets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tweet_lens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">x_train_idx</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Max tweet word length: &#39;</span><span class="p">,</span><span class="n">tweet_lens</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Mean tweet word length: &#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">tweet_lens</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;99% percent under: &#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">tweet_lens</span><span class="p">,</span><span class="mf">0.99</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Max tweet word length:  229
Mean tweet word length:  15.0
99% percent under:  37.0
</pre></div>
</div>
</div>
</div>
<p>We cut the sequences which are larger than our chosen maximum length (<code class="docutils literal notranslate"><span class="pre">max_lenght</span></code>) and fill with zeros the ones that are shorter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="c1"># We choose the max length</span>
 <span class="n">max_length</span> <span class="o">=</span> <span class="mi">40</span>

<span class="c1"># A function to make all the sequence have the same lenght</span>
<span class="c1"># Note that the output is a Numpy matrix</span>
 <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">):</span>
  <span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">seq_len</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
    <span class="n">len_tweet</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">len_tweet</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">len_tweet</span> <span class="o">&lt;=</span> <span class="n">seq_len</span><span class="p">:</span>
        <span class="c1"># If its shorter, we fill with zeros (the padding Token index)</span>
        <span class="n">features</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">tweet</span><span class="p">):]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tweet</span><span class="p">)[:</span><span class="n">seq_len</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">len_tweet</span> <span class="o">&gt;</span> <span class="n">seq_len</span><span class="p">:</span>
        <span class="c1"># If its larger, we take the last &#39;seq_len&#39; indexes</span>
        <span class="n">features</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tweet</span><span class="p">)[</span><span class="o">-</span><span class="n">seq_len</span><span class="p">:]</span>
  <span class="k">return</span> <span class="n">features</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We convert our list of tokens into a numpy matrix</span>
<span class="c1"># where all instances have the same lenght</span>
<span class="n">x_train_pad</span> <span class="o">=</span> <span class="n">padding</span><span class="p">(</span><span class="n">x_train_idx</span><span class="p">,</span><span class="n">max_length</span><span class="p">)</span>
<span class="n">x_test_pad</span> <span class="o">=</span> <span class="n">padding</span><span class="p">(</span><span class="n">x_test_idx</span><span class="p">,</span><span class="n">max_length</span><span class="p">)</span>

<span class="c1"># We convert our target list a numpy matrix</span>
<span class="n">y_train_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">some_number</span> <span class="o">=</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before padding: &#39;</span><span class="p">,</span> <span class="n">x_train_idx</span><span class="p">[</span><span class="n">some_number</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After padding: &#39;</span><span class="p">,</span> <span class="n">x_train_pad</span><span class="p">[</span><span class="n">some_number</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before padding:  [1, 3, 71, 24, 122, 3, 533, 74, 13, 4, 3, 102, 13, 209, 2, 12, 150, 4, 22, 5, 18, 667, 3, 138, 61, 7, 3296, 4]
After padding:  [   0    0    0    0    0    0    0    0    0    0    0    0    1    3
   71   24  122    3  533   74   13    4    3  102   13  209    2   12
  150    4   22    5   18  667    3  138   61    7 3296    4]
</pre></div>
</div>
</div>
</div>
<p>Now, let’s convert the data to pytorch format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create Tensor datasets</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_train_pad</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train_np</span><span class="p">))</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_test_pad</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_test_np</span><span class="p">))</span>

<span class="c1"># Batch size (this is an important hyperparameter)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># dataloaders</span>
<span class="c1"># make sure to SHUFFLE your data</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">drop_last</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_data</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">drop_last</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Each batch of data in our traning proccess will have the folllowing format:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain one batch of training data</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sample input size: &#39;</span><span class="p">,</span> <span class="n">sample_x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># batch_size, seq_length</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sample input: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sample_x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sample input: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sample_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sample input size:  torch.Size([100, 40])
Sample input: 
 tensor([[    0,     0,     0,  ...,     4,     4,     4],
        [    0,     0,     0,  ...,  7447, 14027,     2],
        [    0,     0,     0,  ...,   100, 22241,     4],
        ...,
        [    0,     0,     0,  ...,  2702,  4409,     2],
        [    0,     0,     0,  ...,   162,    17,     1],
        [    0,     0,     0,  ...,    67, 12904,    49]])
Sample input: 
 tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
</pre></div>
</div>
</div>
</div>
<p>Now, we will define the <code class="docutils literal notranslate"><span class="pre">SentimentRNN</span></code> class. Most of the model’s class will be familiar to you, but there are two important layers we would like you to pay attention to:</p>
<ul class="simple">
<li><p>Embedding Layer</p></li>
</ul>
<blockquote>
<div><p>This layer is like a linear layer, but it makes it posible to use a sequence of inedexes as inputs (instead of a sequence of one-hot-encoded vectors). During training, the Embedding layer learns a linear transformation from the space of words (a vector space of dimension <code class="docutils literal notranslate"><span class="pre">num_words_dict</span></code>) into the a new, smaller, vector space of dimension <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code>. We suggest you to read this <a class="reference external" href="https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518/3">thread</a> and the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">pytorch documentation</a> if you want to learn more about this particular kind of layers.</p>
</div></blockquote>
<ul class="simple">
<li><p>LSTM layer</p></li>
</ul>
<blockquote>
<div><p>This is one of the most used class of Recurrent Neural Networks. In Pytorch we can add several stacked layers in just one line of code. In our case, the number of layers added are decided with the parameter <code class="docutils literal notranslate"><span class="pre">no_layers</span></code>. If you want to learn more about LSTMs we strongly recommend you this <a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colahs thread</a> about them.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SentimentRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">no_layers</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">drop_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SentimentRNN</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_layers</span> <span class="o">=</span> <span class="n">no_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>

    <span class="c1"># Embedding Layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

    <span class="c1"># LSTM Layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">hidden_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span>
                        <span class="n">num_layers</span><span class="o">=</span><span class="n">no_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span><span class="p">)</span>

    <span class="c1"># Dropout layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_prob</span><span class="p">)</span>

    <span class="c1"># Linear and Sigmoid layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sig</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">hidden</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Embedding out</span>
    <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1">#Shape: [batch_size x max_length x embedding_dim]</span>

    <span class="c1"># LSTM out</span>
    <span class="n">lstm_out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
    <span class="c1"># Shape: [batch_size x max_length x hidden_dim]</span>

    <span class="c1"># Select the activation of the last Hidden Layer</span>
    <span class="n">lstm_out</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="c1"># Shape: [batch_size x hidden_dim]</span>

    <span class="c1">## You can instead average the activations across all the times</span>
    <span class="c1"># lstm_out = torch.mean(lstm_out, 1).contiguous()</span>

    <span class="c1"># Dropout and Fully connected layer</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="c1"># Sigmoid function</span>
    <span class="n">sig_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sig</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="c1"># return last sigmoid output and hidden state</span>
    <span class="k">return</span> <span class="n">sig_out</span><span class="p">,</span> <span class="n">hidden</span>

  <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; Initializes hidden state &#39;&#39;&#39;</span>
    <span class="c1"># Create two new tensors with sizes n_layers x batch_size x hidden_dim,</span>
    <span class="c1"># initialized to zero, for hidden state and cell state of LSTM</span>
    <span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">no_layers</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">no_layers</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span><span class="n">c0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
</div>
<p>We choose the parameters of the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters of our network</span>

<span class="c1"># Size of our vocabulary</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="n">num_words_dict</span>

<span class="c1"># Embedding dimension</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Number of stacked LSTM layers</span>
<span class="n">no_layers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Dimension of the hidden layer in LSTMs</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Dropout parameter for regularization</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Dropout parameter for regularization</span>
<span class="n">drop_prob</span> <span class="o">=</span> <span class="mf">0.25</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s define our model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentimentRNN</span><span class="p">(</span><span class="n">no_layers</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span>
                     <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="n">drop_prob</span><span class="p">)</span>
<span class="c1"># Moving to gpu</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SentimentRNN(
  (embedding): Embedding(30000, 32)
  (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.25)
  (dropout): Dropout(p=0.25, inplace=False)
  (fc): Linear(in_features=64, out_features=1, bias=True)
  (sig): Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># How many trainable parameters does our model have?</span>
<span class="n">model_parameters</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model_parameters</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Number of parameters: &#39;</span><span class="p">,</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total Number of parameters:  1018433
</pre></div>
</div>
</div>
</div>
<p>We choose the losses and the optimizer for the training procces.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># loss and optimization functions</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Binary crossentropy is a good loss function for a binary classification problem</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

<span class="c1"># We choose an Adam optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># function to predict accuracy</span>
<span class="k">def</span> <span class="nf">acc</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">label</span><span class="p">):</span>
  <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">label</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We are ready to train our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of training Epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Maximum absolute value accepted for the gradeint</span>
<span class="n">clip</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Initial Loss value (assumed big)</span>
<span class="n">valid_loss_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Inf</span>

<span class="c1"># Lists to follow the evolution of the loss and accuracy</span>
<span class="n">epoch_tr_loss</span><span class="p">,</span><span class="n">epoch_vl_loss</span> <span class="o">=</span> <span class="p">[],[]</span>
<span class="n">epoch_tr_acc</span><span class="p">,</span><span class="n">epoch_vl_acc</span> <span class="o">=</span> <span class="p">[],[]</span>

<span class="c1"># Train for a number of Epochs</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">train_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

  <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>

    <span class="c1"># Initialize hidden state</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="c1"># Creating new variables for the hidden state</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># Move batch inputs and labels to gpu</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set gradient to zero</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Compute model output</span>
    <span class="n">output</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>

    <span class="c1"># Calculate the loss and perform backprop</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># calculating accuracy</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">acc</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">train_acc</span> <span class="o">+=</span> <span class="n">accuracy</span>

    <span class="c1">#`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


  <span class="c1"># Evaluate on the validation set for this epoch</span>
  <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">val_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">valid_loader</span><span class="p">:</span>

    <span class="c1"># Initialize hidden state</span>
    <span class="n">val_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">val_h</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">val_h</span><span class="p">])</span>

    <span class="c1"># Move batch inputs and labels to gpu</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Compute model output</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">val_h</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">val_h</span><span class="p">)</span>

    <span class="c1"># Compute Loss</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">acc</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">+=</span> <span class="n">accuracy</span>

  <span class="n">epoch_train_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>
  <span class="n">epoch_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">val_losses</span><span class="p">)</span>
  <span class="n">epoch_train_acc</span> <span class="o">=</span> <span class="n">train_acc</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">epoch_val_acc</span> <span class="o">=</span> <span class="n">val_acc</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">epoch_tr_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_train_loss</span><span class="p">)</span>
  <span class="n">epoch_vl_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_val_loss</span><span class="p">)</span>
  <span class="n">epoch_tr_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_train_acc</span><span class="p">)</span>
  <span class="n">epoch_vl_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_val_acc</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;train_loss : </span><span class="si">{</span><span class="n">epoch_train_loss</span><span class="si">}</span><span class="s1"> val_loss : </span><span class="si">{</span><span class="n">epoch_val_loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;train_accuracy : </span><span class="si">{</span><span class="n">epoch_train_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1"> val_accuracy : </span><span class="si">{</span><span class="n">epoch_val_acc</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">epoch_val_loss</span> <span class="o">&lt;=</span> <span class="n">valid_loss_min</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Validation loss decreased (</span><span class="si">{:.6f}</span><span class="s1"> --&gt; </span><span class="si">{:.6f}</span><span class="s1">).  Saving model ...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_loss_min</span><span class="p">,</span><span class="n">epoch_val_loss</span><span class="p">))</span>
    <span class="c1"># torch.save(model.state_dict(), &#39;../working/state_dict.pt&#39;)</span>
    <span class="n">valid_loss_min</span> <span class="o">=</span> <span class="n">epoch_val_loss</span>
  <span class="nb">print</span><span class="p">(</span><span class="mi">25</span><span class="o">*</span><span class="s1">&#39;==&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1
train_loss : 0.4367361353733577 val_loss : 0.39174133955966683
train_accuracy : 79.530625 val_accuracy : 82.3628125
Validation loss decreased (inf --&gt; 0.391741).  Saving model ...
==================================================
Epoch 2
train_loss : 0.3765802335098851 val_loss : 0.3724124691961333
train_accuracy : 83.19140625 val_accuracy : 83.42031250000001
Validation loss decreased (0.391741 --&gt; 0.372412).  Saving model ...
==================================================
Epoch 3
train_loss : 0.35746844720793886 val_loss : 0.365050206175074
train_accuracy : 84.16882812499999 val_accuracy : 83.7440625
Validation loss decreased (0.372412 --&gt; 0.365050).  Saving model ...
==================================================
Epoch 4
train_loss : 0.34491546426317654 val_loss : 0.36467386982403693
train_accuracy : 84.879140625 val_accuracy : 83.77
Validation loss decreased (0.365050 --&gt; 0.364674).  Saving model ...
==================================================
Epoch 5
train_loss : 0.33429012800217606 val_loss : 0.36189084346871825
train_accuracy : 85.44296875 val_accuracy : 84.0221875
Validation loss decreased (0.364674 --&gt; 0.361891).  Saving model ...
==================================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epoch_tr_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train Acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epoch_vl_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epoch_tr_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epoch_vl_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/sentiment_analysis_73_0.png" src="../../_images/sentiment_analysis_73_0.png" />
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="what-s-next">
<h1>What’s Next?<a class="headerlink" href="#what-s-next" title="Permalink to this headline">¶</a></h1>
<p>You can use this project template as a starting point to think about your own project. There are a lot of ways to continue, here we share with you some ideas you migth find useful:</p>
<ul class="simple">
<li><p><strong>Work on the Preproccesing.</strong> We used a very rudimentary way to tokenize tweets. But there are better ways to preprocess the data. Can you think of a suitable way to preprocess the data for this particular task? How does the performance of the model change when the data is processed correctly?</p></li>
<li><p><strong>Work on the Model.</strong> The RNN model proposed in this notebook is not optimized at all. You can work on finding a better architecture or better hyperparamenters. May be using bidirectonal LSTMs or increasing the number of stacked layers can improve the performance, feel free to try different approaches.</p></li>
<li><p><strong>Work on the Embedding.</strong> Our model learnt an embedding during the training on this Twitter corpus for a particular task. You can explore the representation of different words in this learned embedding. Also, you can try using different word embeddings. You can train them on this corpus or you can use an embedding trained on another corpus of data. How does the change of the embedding affect the model performance?</p></li>
<li><p><strong>Try sentiment analysis on another dataset.</strong> There are lots of available dataset to work with, we can help you find one that is interesting to you. Do you belive that a sentiment analysis model trained on some corpus (Twitter dataset) will perform well on another type of data (for example, youtube comments)?</p></li>
</ul>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./projects/NaturalLanguageProcessing"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ideas_and_datasets.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ideas</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="machine_translation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Translation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Neuromatch<br/>
  
    <div class="extra_footer">
      <div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>